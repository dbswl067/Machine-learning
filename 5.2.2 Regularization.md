# 5.2.2 Regularization
- 모델 복잡도에 대한 패널티로 정규화는 Over-fitting을 예방하고 Generalization(일반화) 성능을 높이는데 도움을 준다.
- Regularization 방법으로는 L1 Regularization, L2 Regularization, Dropout, Early stopping 등이 있다.

### L1 Regularization
L1 규제는 가중치의 제곱의 합이 아닌 가중치의 합을 더한 값에 규제 강도(Regularization Strength) λ를 곱하여 오차에 더한다. 이렇게 하면 L2 규제와는 달리 어떤 가중치는 실제로 0이 된다. 즉, 모델에서 완전히 제외되는 특성이 생기는 것이다. 일부 계수를 0으로 만듦으로써 모델을 이해하기 쉬워지고, 모델의 가장 중요한 특성이 무엇인지 드러나는 것이다. 그러나 L2 규제가 L1 규제에 비해 더 안정적이라 일반적으로는 L2규제가 더 많이 사용된다.

### L2 Regularization
각 가중치 제곱의 합에 규제 강도(Regularization Strength) λ를 곱한다. 그 값을 손실함수에 더한다. λ를 크게 하면 가중치가 더 많이 감소되고(규제를 중요시함), λ를 작게 하면 가중치가 증가한다(규제를 중요시하지 않음). 가중치를 갱신할 때, 손실함수의 미분값을 이전 가중치에서 빼서 다음 가중치를 계산한다. 따라서 가중치가 크면 손실 함수가 커지고, 다음 가중치가 크게 감소된다. 

### Dropout
![](https://t1.daumcdn.net/cfile/tistory/99B3AC365AAB78EA11)
- 드롭아웃 기법은 신경망 모델에서 뉴런을 임의로 삭제하면서 학습하는 방법이다. 학습 시에 은닉층(Hidden Layer)의 뉴런을 무작위로 골라 삭제한다. 삭제된 뉴런은 위의 그림과 같이 신호를 전달하지 않는다. 훈련 때에는 데이터를 흘릴 때마다 삭제할 뉴런을 무작위로 선택하고, 시험 때는 모든 뉴런에 신호를 전달한다. 단, 시험 때는 각 뉴런의 출력에 훈련 때 삭제한 비율을 곱하여 출력한다.

- 즉, 뉴런을 임의로 삭제하여 적은 수의 뉴런만으로 지정된 레이블을 맞추도록 훈련하는 것이다. 이렇게 훈련된 뉴런들은 테스트 시에 전체 뉴런이 활성화되면, 일반적으로 드롭아웃 없이 학습된 뉴런에 비해 더 정답을 잘 찾는다는 것이 드롭아웃의 아이디어이다. 다만, 적은 수의 뉴런만으로 학습을 해야하므로 학습 시간이 오래걸린다는 단점이 존재한다.

![](https://t1.daumcdn.net/cfile/tistory/995336495AAB7B610D)
- 좌측 그래프는 드롭아웃없이 학습한 모델의 성능을, 오른쪽 그래프는 드롭아웃을 적용한 모델의 성능을 나타낸다. 그림과 같이 드롭아웃을 적용하니 훈련 데이터와 시험 데이터에 대한 정확도 차이가 준 것을 볼 수 있다. 이처럼 드롭아웃을 적용하면 오버피팅을 억제할 수 있다.


### Early stopping(조기 종료)
Epoch 수가 클수록 Training set에 대한 오차는 줄어들지만 Epoch 수가 늘어날 수록 오차가 증가하며 Overfitting이 발생할 수 있다. 따라서, 이전 Epoch와 비교해서 오차가 증가하면 Overfitting이 발생하기 전에 멈추는 것을 'Early stopping'이라고 한다.
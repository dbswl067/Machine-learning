# Hidden Units
- hidden layer의 설계는 활발하게 연구되고 있는 내용이긴 하지만, 아직까지는 절대적인 이론적 가이드라인이 있지는 않다.
- Rectified linear unit (ReLU)가 기본적으로 hidden unit으로 많이 쓰인다.
- 현재는 validation set을 사용하거나 시행착오를 통해 결정하는 경우가 대부분이다.

## Rectiﬁed Linear Units and Their Generalizations
Rectiﬁed Linear Units은 g(z)=max{0,z} 을 activation function으로 사용한다. 이건 linear unit과 유사하여 최적화 하기 용이하다. 차이점이라 하면 절반은 결과값이 0으로 죽어버린다는 것이다. 미분값은 active unit에서는 상수로 일정하고, 이차미분값은 모두 0이다.

![ReLU](https://mlnotebook.github.io/img/transferFunctions/relu.png)
- x>0  이면 기울기가 1인 직선이고, x<0이면 함수값이 0이된다.
- sigmoid, tanh 함수와 비교시 학습이 훨씬 빨라진다.
- 연산 비용이 크지않고, 구현이 매우 간단하다.
- x<0인 값들에 대해서는 기울기가 0이기 때문에 뉴런이 죽을 수 있는 단점이 존재한다.

h = g(W^Tx + b) 수식처럼 보통 active function은 affine matrix 후에 적용된다. 이때 b는 0.1처럼 작은 양수로 초기값을 설정한다.

몇몇 ReLU 보다 더 성능이 좋은 버전이 있다.
### Leaky ReLU
Leaky ReLU는 ReLU와 거의 비슷한 형태를 갖는다. 입력 값이 음수일 때 완만한 선형 함수를 그려준다. 일반적으로 알파를 0.01로 설정한다. (그래프에서는 시각화 편의상 알파를 0.1로 설정)
![Leaky ReLU](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcKWXWJ%2FbtqDHtiKHJP%2FB0pplOiUJmryTMkmwHgwn1%2Fimg.png)

### PReLU(parametric rectified linear unit)
LeakyReLU와 거의 유사한 형태를 보인다. 하지만 Leaky ReLU에서는 알파 값이 고정된 상수였던 반면에 PReLU에서는 학습이 가능한 파라미터로 설정된다.
![PReLU](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbCNvJe%2FbtqDEFZbNk6%2F7v6WXzQSFsZytb5xAKBSK0%2Fimg.png)

### Maxout
이 함수는 ReLU가 가지는 모든 장점을 가졌으며, dying ReLU문제 또한 해결한다. 하지만 계산량이 복잡하다는 단점이 있다.
그리고 아편변환을 한 레이어에서 maxpooling을 거친 것과 같은데 아래 그림을 보면 더 잘 알 수 있다.
![maxpooling1](https://leejunhyun.github.io/assets/img/Deeplearning/DLB/06/DLB-06-04.png)  
d차원의 input feature에서 m차원의 output을 낼 때, k개의 affine transform 결과중 가장 큰 값을 하나 뽑아내는 것이다. 간단한 예시를 위해 2차원 input feature에서 1차원 output을 낼때, 3개의 affine transform 중 가장 큰 값을 하나 뽑아내는 maxout unit을 보면 아래 그림과 같다.
![maxpooling2](https://leejunhyun.github.io/assets/img/Deeplearning/DLB/06/DLB-06-05.png)
이를 이용해서 f(x) = x^2를 근사한다면 아래 그림과 같다.
![f(x) = x^2](https://leejunhyun.github.io/assets/img/Deeplearning/DLB/06/DLB-06-06.png)
구간에 따라 파란색, 초록색, 빨간색 값중 큰 값을 선택한다. k의 수가 더 커질수록 세분화되어 원래 함수에 근사할 수 있다. 빨간색 값을 선택한 경우에도 초록색, 파란색의 feature 정보는 잃지 않는다는 장점이 있다. Maxout은 성능이 좋지만, 다만 트레이닝 셋이 크지 않거나 unit당 pieces 수가 많은 경우에는 ReLU 보다 regularization이 필요하다.

## Logistic Sigmoid and Hyperbolic Tangent
ReLU 이전에는 대부분 sigmoid activation function이나  hyperbolic tangent activation function를 사용했다. 두 함수는 서로 유사하다.
- tanh(x)=2σ(2x)−1
- tanh(x)=ex−e−xex+e−x
- tanh′(x)=1−tanh2(x)
![tanh](https://mlnotebook.github.io/img/transferFunctions/tanh.png)
![tanh derivative](https://mlnotebook.github.io/img/transferFunctions/dtanh.png)
이 함수들은 saturation 때문에 hidden unit 으로는 잘 사용되지 않는다. 다만 saturation을 상쇄해줄 cost function 을 사용할 때 output의 함수로 종종 사용된다.

## Other Hidden Units
### Radial basis function(RBF)
![RBF](https://keepmind.net/wp-content/uploads/2019/11/ML-16-04-1024x576.png)
x가 template Wn에 가까워질수록 큰 값을 갖는다. 대부분의 x에 대해 0으로 saturation되기 때문에 최적화 하기 어렵다.

### Softplus
ReLU 함수를 부드럽게 깎아놓은 형태를 취한다. 모든 구간에서 미분가능하고 덜 saturation되기 때문에 성능이 잘 나올 것 같지만 실험적으로는 잘 안된다.
![Softplus](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FXAppd%2FbtqDHsqB1Of%2FdHAZTfc51ykw5d59Dtj2bK%2Fimg.png)

### Hard tanh
max(−1,min(1,a))  으로 정의되며 tanh와 rectifier와 비슷하다.
![Hard tanh](https://atcold.github.io/pytorch-Deep-Learning/images/week11/11-1/Hardtanh.png)

### Reference
- [딥러닝에서 사용하는 활성화함수](https://reniew.github.io/12/)
- [갈아먹는 딥러닝 기초 [1] Activation Function(활성화 함수) 종류](https://yeomko.tistory.com/39)
- [Radial Basis Function](https://keepmind.net/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-radial-basis-function/)
- [RBF](https://analysisbugs.tistory.com/164)
# 5.8 Unsupervised Learning Algorithms
> 지도학습과는 달리 정답 라벨이 없는 데이터를 비슷한 특징끼리 군집화하여 새로운 데이터에 대한 결과를 예측하는 방법을 비지도학습이라고 한다.

![비지도 학습 개념](http://blog.skby.net/blog/wp-content/uploads/2019/09/Unsupervised-Learning-Cencept.png)
- 입력데이터에 대한 목표값 없이 데이터가 어떻게 구성되었는지를 알아내는 기계학습 기법

Unsupervised learning은 Supervision signal(label, target)이 아닌 오직 feature에 대한 것이다. 그러나 이 둘은 수식적으로 엄밀하게 구분되어있는 것은 아니다. Unsupervised learning은 distribution으로부터 뽑아내는 정보가 사람이 매겨준 annotation, target, label이 필요치 않은 것을 말한다.   

가장 고전적인 unsupervised learning task는 데이터의 “best” representation을 찾는 것이다. “best” representation을 찾는다는 것은 원본 feature x에 대한 정보를 잃지 않으면서 원본보다 더 단순하거나 이용하기 쉽게 만드는 representation을 찾는다는 것이다. 단순한 representation으로 바꿔주는 방법에는 주로 세가지가 쓰인다. 

1. Low-dimensional representations: 원본보다 더 작은 차원으로 데이터를 압축하여 데이터를 표현하는 것
2. Sparse representations: dataset을 input의 대부분을 0으로 만드는 representation으로 바꾼다. 이렇게 바꾸려면, 원본보다 차원을 늘려줘야 정보를 잃지 않으면서 sparse한 reapresentation을 만들 수 있다.
3. Independent representations: 통계적으로 독립적인 데이터 소스를 각각의 차원으로 분리해낸다.

## 주성분 분석(PCA, Principle Component Analysis)
> 주성분 분석은 특성들이 통계적으로 상관관계가 없도록 데이터셋을 회전시키는 기술이다. 회전한 뒤에 데이터를 설명하는데 얼마나 중요하냐에 따라 특성 일부만 선택된다.

- 기존의 데이터는 속성 하나하나가 좌표축으로 이루어진 다차원 공간에 위치하고 있다.
- 속성들은 둘 간에 서로 연관되어 있는데 이를 수치화한 것을 상관계수(Correlation Coefficient)라고 한다. 이를 확장하여 모든 속성들을 고려했을 때, 가장 전체 데이터를 잘 표현할 수 있는 방향(벡터)를 찾을 수 있을 것이다.
- 이렇게 모든 속성에서 가장 중요한 방향(주성분)을 찾아 나가는 것을 PCA 라고 한다. 다르게 표현하면 전체 데이터에서 가장 분산이 큰 방향 을 찾는 것이다.
- 첫번째 주성분을 찾으면 그것과 직각이 되는 방향 중에서 가장 분산이 큰 성분을 찾는다. 이렇게 원본 속성 만큼의 주성분을 차례대로 찾아나갈 수 있다.

![PCA](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcMG9qd%2FbtqDwdaxyOW%2FunQy21qDV9VX2UCGvv65s1%2Fimg.png)

- 그림1
	- 왼쪽 위 그래프에서 분산이 가장 큰 방향을 찾는다. 그것이 성분 1이다. 이 방향은 데이터에서 가장 많은 정보를 담고있다. 즉, 특성들의 상관관계가 가장 큰 방향이다.
	- 그 후 첫 번째 방향과 직간인 방향 중에서 가장 많은 정보를 담은 방향을 찾는다.  해당 그래프에서 주성분을 하나로 선정한다면 2번째 성분은 제거된다.
	- 그리고 다시 원래 모양으로 회전시킨다. 

- 그림2 
	- 주성분 1과 2를 각각 x출과 y축에 나란하게 회전
- 그림3
	- 주성분 일부만 남기는 차원 축소로 첫번째 주성분만 유지
- 그림4
	- 그림 3에서 데이터에 평균을 더해 반대로 회전

이러한 변환은 데이터에서 노이즈를 제거하거나 주성분에서 유지되는 정보를 시각화는데 종종 사용된다.

## k-means Clustering
> k-means clustering은 데이터들을 유사한 것들끼리 묶어서 k개의 cluster로 표현하는 것이다.

데이터를 k차원의 one-hot 벡터로 나타내는 것으로도 볼 수 있다. 유사한 데이터들을 하나의 클러스터 원핫으로 표현하는 것은 정보의 손실이 있긴 하지만, 많은 데이터들을 하나의 정수로 표현가능하다는 장점이 있다.

k-means clustering은 k개의 각기다른 centroid 들 {μ(1),...,μ(k)}을 초기화하는 것으로 시작된다. 그 다음 아래 두 단계를 수렴할때까지 반복한다.

1. 각각의 학습데이터를 가장 가까운 cluster μ(i) 에 할당한다.
2. 각각의 cluster centroid를 할당된 데이터 포인트들의 평균으로 이동시킨다.

### K-means 알고리즘 특징
- 거리기반 분류: 중심점과의 유클리디안 거리 최소화
- 반복 작업: 초기의 잘못된 병합을 알고리즘을 반복 수행하여 회복
- 짧은 계산시간: 간단한 알고리즘이며, 대규모 적용 가능
- 탐색적 기법: 주어진 자료에 대한 사전정보 없이 의미있는 자료구조를 찾아낼 수 있음

![K-means 알고리즘의 원리](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile22.uf.tistory.com%2Fimage%2F22446C4E57FE2FC611BDA2)
- 군집의 수 K를 정의 및 초기 K개 군집의 중심(Centroid) 선택
- 각 관측 값들을 가장 가까운 중심의 군집에 할당
- 새로운 군집의 중심 계산
- 재정의 된 중심값 기준으로 다시 거리기반의 군집 재분류, 경계가 변경되지 않으면 종료

### K-means 알고리즘의 장점
- 탐색적 기법: 군집분석은 그 자체가 대용량 데이터에 대한 탐색적인 기법으로서, 주어진 데이터의 내부구조에 대한 사전적인 정보 없이 의미있는 자료구조를 찾아낼 수 있는 방법
- 다양한 형태의 데이터에 적용 가능: 분석을 위해서는 기본적으로 관찰치 간의 거리를 데이터형에 맞게만 정의하면, 거의 모든 형태의 데이터에 대하여 적용이 가능한 방법
- 분석방법의 적용 용이성: 대부분의 군집방법이 분석대상 데이터에 대해 사전정 보를 거의 요구하지 않음 적용 유리 즉, 모형화를 위한 분석과 같이 사전에 특정 변수에 대한 역할 정의가 필요하지 않고 다만 관찰치 들 사 이의 거리만이 분석에 필요한 입력자료로 사용.

### K-means 알고리즘 단점
- 가중치와 거리 정의: 군집분석의 결과는 관찰치 사이의 비 유사성인 거리 또는 유사성을 어떻게 정의하느냐에 따라 크게 좌우, 따라서 관찰치 들 사이의 거리를 정의하고 각 변수에 대한 가중치를 결정하는 것은 매우 어려운 문제다
- 초기 군집 수의 결정: k-평균 군집분석에서는 사전에 정의된 군집수를 기준 으로 동일한 수의 군집을 찾게 되므로 만일 군집수 k 가 원 데이터구조에 적합하지 않으면 좋은 결과를 얻을 수 없음
- 결과해석의 어려움: 탐색적인 분석방법으로의 장점을 가지고 있는 반면에 사전에 주어진 목적이 없으므로 결과를 해석하는데 있어서 어려움 존재

### K-means 알고리즘의 활용 분야
- Data Mining에서 데이터 분류 및 군집 알고리즘으로 활용
- 트랜드 또는 성향이 불분명한 시장을 분석하는 경우
- 시장과 고객 분석, 패턴인식, 공간데이터 분석, Text Mining 등
- 최근에는 패턴인식, 음성인식의 기본 알고리즘으로 활용 
- 개체가 불규칙적이고, 개체간 관련성을 정확히 알 수 없는 분류 초기 단계

## Reference
- [인공지능 지도학습, 비지도학습, 강화학습](https://ebbnflow.tistory.com/165)
- [머신러닝 비지도학습과 군집화, 분포, 분류, 타당성 평가](https://ikkison.tistory.com/51)
- [비지도학습 PCA, K-means](https://medium.com/mighty-data-science-bootcamp/%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5%EC%9D%98-%EB%AA%A8%EB%93%A0-%EA%B2%83-29ec2aceb56e)
- [비지도학습과 클러스터링](https://yamalab.tistory.com/47)
- [비지도학습-PCA](https://duckkkk.com/entry/%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5-PCA-%EC%A3%BC%EC%84%B1%EB%B6%84-%EB%B6%84%EC%84%9D)
- [비지도 변환 : 주성분 분석](https://kolikim.tistory.com/27)
- [클러스터링(Clustering)을 통한 데이터 분류기법, K-평균(K-Means) 알고리즘](https://needjarvis.tistory.com/140)
- [머신러닝 k-means Clustering 특징, 장단점, 적용 예시](https://muzukphysics.tistory.com/146)
- [비지도 학습이란](http://blog.skby.net/%EB%B9%84%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5-unsupervised-learning/)


# 5.10 Building a Machine Learning Algorithm
머신러닝 알고리즘을 만드는 방법은 상당히 단순하다. 데이터셋과 cost function을 연결시켜주고, optimization 종류와 모델을 결정한다.

예를 들어 linear regression에서는 데이터셋(X,y)과 cost function 을 아래와 같이 엮어준다.

대부분의 경우엔 normal equation을 이용해서 cost의 gradient가 0인 지점으로 최적화 한다.

위의 예시에서 설명한 각각의 요소들을 다른 것으로 대체함으로써 많은 알고리즘을 설명할 수 있다. cost function을 negative log likelihood 처럼 통계적인 추정을 할 수 있는 항을 포함하게끔 바꿀 수도 있다. 또는 regularization 을 위한 항을 추가할 수도 있다. 예를 들어 weight decay 를 linear regression에 추가하면 아래와 같다.

위 식에서는 여전히 closed form optimization 이지만, 모델이 non linear로 바뀌게 되면 cost function은 더이상 closed form 으로 최적화 되지 않는다. 따라서 gradient descent 같은 반복적인 과정으로 최적화 해야한다.

이처럼 model, cost, optimization 을 엮어서 학습 알고리즘을 만드는 것은 supervised learning과 unsupervised learning에 모두 사용할 수 있다. unsupervised learning은 정답 레이블 y가 없으므로 오로지 데이터 X와 적절한 unsupervised cost로 정의된다. 예를 들어, 첫번째 PCA 벡터는 아래와 같은 cost function으로 얻을 수 있다.

이때 r은 reconstruction function r(x)=w⊤xw 이다. 즉, PCA로 데이터를 저차원으로 임베딩 했다가 r 함수로 다시 복원했을 때, 원본 데이터의 정보 손실을 최소화 하는 것이다.

많은 머신러닝 알고리즘들이 이런 방식으로 만들어진다. 그러나 가끔은 특별한 optimizer를 사용해야 할 때도 있다. 예를 들면 decision tree나 k-means 같은 경우는 cost function이 flat하기 때문에 gradient based optimization은 적합하지 않다.
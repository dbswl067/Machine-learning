# 5.10 Building a Machine Learning Algorithm
머신러닝 알고리즘을 만드는 방법은 상당히 단순하다. 데이터셋과 cost function을 연결시켜주고, optimization 종류와 모델을 결정한다.

예를 들어 linear regression에서는 데이터셋(X,y)과 cost function 을 아래와 같이 엮어준다.

![linear regression cost function](./image/linearRegressionCostFunction.png)

대부분의 경우엔 normal equation을 이용해서 cost의 gradient가 0인 지점으로 최적화 한다.

위의 예시에서 설명한 각각의 요소들을 다른 것으로 대체함으로써 많은 알고리즘을 설명할 수 있다. cost function을 negative log likelihood 처럼 통계적인 추정을 할 수 있는 항을 포함하게끔 바꿀 수도 있다. 또는 regularization 을 위한 항을 추가할 수도 있다. 예를 들어 weight decay 를 linear regression에 추가하면 아래와 같다.

위 식에서는 여전히 closed form optimization 이지만, 모델이 non linear로 바뀌게 되면 cost function은 더이상 closed form 으로 최적화 되지 않는다. 따라서 gradient descent 같은 반복적인 과정으로 최적화 해야한다.

이처럼 model, cost, optimization 을 엮어서 학습 알고리즘을 만드는 것은 supervised learning과 unsupervised learning에 모두 사용할 수 있다. unsupervised learning은 정답 레이블 y가 없으므로 오로지 데이터 X와 적절한 unsupervised cost로 정의된다. 예를 들어, 첫번째 PCA 벡터는 아래와 같은 cost function으로 얻을 수 있다.

![PCA벡터](./image/PCA벡터.png)

이때 r은 reconstruction function r(x)=w⊤xw 이다. 즉, PCA로 데이터를 저차원으로 임베딩 했다가 r 함수로 다시 복원했을 때, 원본 데이터의 정보 손실을 최소화 하는 것이다.

많은 머신러닝 알고리즘들이 이런 방식으로 만들어진다. 그러나 가끔은 특별한 optimizer를 사용해야 할 때도 있다. 예를 들면 decision tree나 k-means 같은 경우는 cost function이 flat하기 때문에 gradient based optimization은 적합하지 않다.
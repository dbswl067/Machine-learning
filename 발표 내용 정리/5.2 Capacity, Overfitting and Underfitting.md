# 5.2 Capacity, Overfitting and Underfitting
## 머신러닝의 중심 과제
- 모델이 교육받은 입력뿐만 아니라 이전에는 볼 수 없었던 새로운 입력에 대해서도 잘 수행해야 한다.
- 즉, 일반화의 오류가 낮기를 원한다는 소리
- 일반화의 오류: 새 입력에 대한 오류의 예상 값
- 우리는 일반적으로 training set과 별도로 수집된 test set에서 성능을 측정하여 기계학습의 일반화 오류를 측정한다.

## 데이터 생성 과정
> i.i.d(independent identically distributed): 각각의 랜덤 변수들이 독립적이고 동일한 확률분포를 가지는 분포를 뜻함. ex)동전, 주사위 던지기

1. i.i.d 가정으로 알려진 일련의 가정을 만든다.
- training error와 test error 사이의 관계를 수학적으로 연구 할 수 있게 해주기 때문

2. 동일한 분포를 사용하여 모든 훈련 예제와 모든 테스트 예제를 생성한다.

## 알고리즘의 성능을 결정하는 요소
1. Training error를 작게 만드는 능력
2. Training error와 Test error의 차이를 작게 만드는 능력

- 이 두 요인은 머신러닝의 중심 과제인 언더피팅과 오버피팅에 해당한다.
- 우리는 모델의 용량을 변경하여 모델의 오버피팅과 언더피팅 가능성을 제어할 수 있다.

## Under-fitting
```
모델이 Traning set에서 충분히 낮은 오차 값을 얻을 수 없을 때 발생
```
"동그란 것은 공이야"라고 로봇에게 가르친다.
![](https://t1.daumcdn.net/cfile/tistory/99E311435B76CC931C)

학습 후, testing 단계에 돌입한다.
![](https://t1.daumcdn.net/cfile/tistory/9953B0385B76CC9411)
![](https://t1.daumcdn.net/cfile/tistory/99FBF5335B76CC9503)
![](https://t1.daumcdn.net/cfile/tistory/99DBD0435B76CC951D)
![](https://t1.daumcdn.net/cfile/tistory/99DD5C4B5B76CC9608)
![](https://t1.daumcdn.net/cfile/tistory/9945E5485B76CC962B)

공 = 동그란 것이라고만 학습되어 error가 발생한다. 이를 too Bias하게 학습된 모델이라고 한다.

## Over-fitting
```
훈련 오류와 시험 오류 사이의 간격이 너무 클 때 발생
```
로봇에게 동그랗고, 먹을 수 없으며 가지고 놀수 있는 것이 공이라고 학습시킨다.
![](https://t1.daumcdn.net/cfile/tistory/995481455B76CC972F)

여기에 공은 실밥이 있고 지름이 7cm이상이라는 지엽적인 특성까지 학습시킨다.
![](https://t1.daumcdn.net/cfile/tistory/99AD89485B76CC9724)

지나치게 학습되어 봤던 데이터는 잘 맞추는데 새로운 데이터는 예측하지 못 하는 결과를 초래했다.
이를 high variance한 모델이라고 한다.

## Under and Over-fitting examples
![](https://i.ytimg.com/vi/dBLZg-RqoLg/maxresdefault.jpg)
- 왼쪽은 Under-fitting의 예시, 오른쪽은 Over-fitting의 예시이다.
- Under-fitting은 학습된 것이 별로 없어 error가 나는 것을 볼 수 있다.
- Over-fitting은 지나치게 학습이 되어 training set의 모든 것을 잘 맞추는 것을 볼 수 있다.

## Bias와 Variance
![](https://t1.daumcdn.net/cfile/tistory/994531365B76CC9801)
- Bias: 실제 값에서 멀어진 척도
- Variance: 예측된 값들이 서로 얼마나 떨어져있는가
- 우리의 목표는 Low Bias하고 Low Variance한 값이다.

### Under-fitting model
![](https://t1.daumcdn.net/cfile/tistory/993980415B76CC9815)
- 일부 특성만 반영하여 편견을 가지고 있음

### Over-fitting model
![](https://t1.daumcdn.net/cfile/tistory/99ED6E335B76CC992C)
- 지엽적인 특성까지 다 반영됨

### Graph
![](https://t1.daumcdn.net/cfile/tistory/99A8AC435B76CC9C20)
- 우리가 찾아야 할 모델은 error가 낮은 중간지점이다.

## Under and Over-fitting 극복하는 방법
### Under-fitting
```
Trade off로써 Bias를 낮추고 Variance를 높이는 전략 사용
```
1. Feature를 더 많이 반영하여 Variance를 높인다.
2. Variance가 높은 머신러닝 모델을 사용한다.

### Over-fitting
```
Trade off로써 bias를 높이고 variance를 낮추는 전략 사용
```
1. 더 많은 데이터 확보하기
2. 모델 복잡도 줄이기
-> Neuron의 hidden  layer를 줄이거나, 노드의 수를 줄이는 방법
3. 가중치 규제 적용하기

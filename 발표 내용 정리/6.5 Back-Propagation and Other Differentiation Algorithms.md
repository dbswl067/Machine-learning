# 6.5 Back-Propagation and Other Differentiation Algorithms
## 오차역전파
ANN을 학습시키기 위한 알고리즘 중 하나이다. 내가 뽑고자 하는 target 값이랑 모델이 예상한 아웃풋 값이 얼마나 차이가 있는지 차이를 먼저 계산한 후에 그 오차 값을 다시 뒤로 전파해 나가면서 각 노드가 가지고 있는 웨이트 값들을 갱신해 나가는 과정이다. 

f(x, y, z) = (x + y)z
q = x + y
f = qz
x = -2, y = 5, z = -4라고 가정한다.

순방향으로 계산해 나가는 것을 Foward Pass라고 한다. 순방향으로 계산해 나갈 때 미분한 값을 구해볼 수 있다. 여기서 '미분을 구한다.'라는 의미는 dq/dx일 때 x가 얼만큼 변했을 때 q는 얼마만큼의 변화량을 갖느냐라는 의미이다. 

 우리가 궁극적으로 구하고자 하는 것은 df/dx이다. dx가 갖는 의미는 x가 1만큼 변했을 때 f가 변하느냐이다. 그러나 직접적으로 구할 수 없다. 체인 룰을 사용해야만 문제를 해결할 수 있다. 체인 룰을 사용한다면 df/dx = df/dq * dq/dx가 된다. df/dq * dq/dx = (-4)(1) = -4가 나오는 것을 볼 수가 있다. 즉, 해석해보자면 x가 1만큼 증가했을 때 f라는 값은 -4만큼 증가했다. (4만큼 감소했다.) df/dy = df/dq * dq/dy = (-4)(1) = -4 

여기서 주목해야 될 점은 x, y, z의 편미분한 값들을 Local Gradient라고 한다. 그리고 앞에서 전달받아 내려온 Gradient 값을 우리는 Global Gradient라고 한다. BackPropagation을 할 때엔 Global Gradient가 역으로 계산 돼서 내려오게 된다. 현재 이전 단계에서 계산된 Global Gradient 값만 있다고 한다면 우리는 쉽게 Loss를 x에 대해 미분한 값을 구할 수 있다. 왜냐하면 Local Gradient와 Global Gradient를 단순하게 곱해주기만 하면 되기 때문이다. 즉, 우리는 Chain Rule을 활용하여 Local Gradient * Global Gradient를 곱하여 계산한다. 이 값들을 곱하게 되면 전체 Loss에 대해서 x에 대한 편미분 값을 Chain Rule에 의거해서 쉽게 구할 수가 있게 된다. 그리고 우리는 Forward Pass 시 Local Gradient를 미리 구하여 저장할 수 있다. 

### 결론
- 아무리 깊고 복잡한 층으로 구성되어 있다 하더라고 Chain Rule을 활용하여 미분 값을 얻어낼 수 있다.
- Forward Pass 시 Local Gradient를 미리 계산하여 저장해둔다.
- 저장해둔 Local Gradient와 Global Gradient를 Backward Pass 시 곱하여 최종 미분 값을 얻는다.

### Reference
- [오차역전파 (Backprogation)의 개념을 쉽게 이해해 봅시다](https://www.youtube.com/watch?v=1Q_etC_GHHk&ab_channel=%ED%85%8C%EB%94%94%EB%85%B8%ED%8A%B8)
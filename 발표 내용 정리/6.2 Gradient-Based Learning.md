# Gradient-Based Learning
뉴럴 넷을 설계하고 학습시키는 것은 gradient descent를 사용한 다른 머신러닝 모델과 크게 다르지 않다. 다른 점이 있다면 뉴럴넷에서 사용하는 nonlinear function들이 loss function을 nonconvex하게 만든다는 것이다. 이때문에 뉴럴넷을 학습할 때는 반복적으로 gradient를 이용하여 파라미터를 조금씩 개선해가며 학습해나가야 한다. 뉴럴넷과 같은 nonconvex optimization에서는 모든 파라미터를 작은 랜덤값으로 초기화 해야 최적값으로 수렴시킬 수 있다.

## Cost Function
cost function은 간단히 negative log-likelihood로 정의되고, training data와 model distribution의 cross-entropy와 같다. 구체적인 형태는 logpmodel을 어떻게 정하느냐에 따라 (모델 p(y∣x) 마다) 다르다. 모델이 saturate될 수 있는 function을 사용한다면 gradient가 flat해지기 때문에 gradient vanishing 문제가 발생할 수 있다. 많은 경우 exp 함수를 사용할 때 매우 낮은 값에서 saturate될 수 있다. 이를 log 함수를 취함으로써, negative log-likelihood는 saturation 문제를 해결할 수 있다